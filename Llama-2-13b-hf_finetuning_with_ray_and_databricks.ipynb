{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd02d6b5-789c-446d-9af1-01e8250d99db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Scaling Llama 2 (7 - 70B) Fine-tuning and Batch Inference on Multi-Node GPUs with Ray AIR on Databricks\n",
    "\n",
    "I will be leveraging Ray AI Runtime (AIR) running on top of Databricks Lakehouse cluster to perform distributed data preprocessing, fine-tuning, hyperparameter tuning, and batch inference using Meta's [Llama-2â€“13b-hf](https://huggingface.co/meta-llama/Llama-2-13b-hf) model from Hugging Face with [Databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset.\n",
    "\n",
    "##1. GettingÂ Started\n",
    "In this section, I will go through the code to explain each step in detail.\n",
    "\n",
    "You have to first request access to Llama 2 models via [Meta website](https://ai.meta.com/resources/models-and-libraries/llama-downloads) and also accept to share your account details with Meta on [Hugging Face website](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf). It typically takes a few minutes or hours to get the access.\n",
    "\n",
    "ðŸš¨ *Note that your Hugging Face account email MUST match the email you provided on the Meta website, or your request will not be approved.*\n",
    "\n",
    "###1.1. Hardware Requirements\n",
    "I ran multiple experiments to find out which instance type can be used for the different model sizes. The following shows the results of the experiments.\n",
    "\n",
    "```\n",
    "+-------------------------+---------------+----------------+----------------+\n",
    "|     Specifications      | Llama-2-7b-hf | Llama-2-13b-hf | Llama-2-70b-hf |\n",
    "+-------------------------+---------------+----------------+----------------+\n",
    "| Batch size per device   |       2       |       1        |       1        |\n",
    "| Context length          |      2048     |      2048      |      2048      |\n",
    "| Instance type           | g5.4xlarge    | g5.4xlarge     | p4d.24xlarge   |\n",
    "| No. of workers          | 8 (Min = 2)   | 8 (Min = 2)    | 8 (Min = 2)    |\n",
    "| No. of GPUs per worker  |       1       |       1        |       8        |\n",
    "| GPU type                |  Nvidia A10G  |  Nvidia A10G   |  Nvidia A100   |\n",
    "| GPU memory              |     24 GB     |     24 GB      |     40 GB      |\n",
    "| CPU cores per worker    |       16      |       16       |       96       |\n",
    "| Memory per worker       |     64 GB     |     64 GB      |     1152 GB    |\n",
    "+-------------------------+---------------+----------------+----------------+\n",
    "```\n",
    "\n",
    "Moreover, you have to choose Databricks runtime version `13.1 ML (GPU, Scala 2.12, Spark 3.4.0)` or above for your cluster as Ray integration support starts from this version in Databricks.\n",
    "\n",
    "###1.2. Installing the libraries\n",
    "Let's begin by installing all required libraries using `%pip install` on the driver and worker nodes. `%pip` magic command in Databricks installs the same package across all nodes in the cluster automatically by running once in your notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cddb7c0-84d3-47d5-b388-37a321fd6b64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install peft==0.5.0\n",
    "%pip install datasets==2.12.0 bitsandbytes==0.40.0 einops==0.6.1 trl==0.4.7\n",
    "%pip install torch==2.1.0 accelerate==0.21.0 transformers==4.31.0 tokenizers==0.13.3\n",
    "%pip install pyarrow==8.0.0\n",
    "%pip install ray[default,rllib,tune]==2.5.1\n",
    "%pip install protobuf==3.20.0\n",
    "%pip install xformers==0.0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4282cc20-0603-4153-9b69-5e3629b689d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "You need to restart the kernel by running the following command to use updated packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceef8119-84c7-492d-b388-58063d25b77e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ff6e968-1eb5-43d7-8da0-3fa920c98abe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###1.2. Setting up imports and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b425282a-b9f1-45c1-bb68-0c4e37037ecf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import torch\n",
    "import transformers\n",
    "import warnings\n",
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "from typing import Any, Dict, List, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster, MAX_NUM_WORKER_NODES\n",
    "from ray.data.preprocessors import BatchMapper\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.air.integrations.mlflow import setup_mlflow\n",
    "\n",
    "transformers.set_seed(42)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25ff8636-7297-4b20-85ff-d75f816ae164",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###1.3. Initializing Ray runtime\n",
    "You have to set up and initialize the Ray cluster according to the size of your Databricks cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51411eed-ee48-46d4-b355-d5478f47156a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_cpu_cores_per_worker = 16 # total cpu's present in each node\n",
    "num_gpu_per_worker = 1 # total gpu's present in each node\n",
    "resource_per_worker_int = (num_cpu_cores_per_worker / num_gpu_per_worker) - 2\n",
    "use_gpu = True\n",
    "username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "ray_log_dir = f\"/local_disk0/user/{username}/ray/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d061472f-9010-43b9-baae-cdbfc4242c26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try: \n",
    "  shutdown_ray_cluster()\n",
    "except:\n",
    "  print(\"No Ray cluster is initiated\")\n",
    "\n",
    "# Start the ray cluster and follow the output link to open the Ray Dashboard - a vital observability tool for understanding your infrastructure and application.\n",
    "setup_ray_cluster(\n",
    "  num_worker_nodes=MAX_NUM_WORKER_NODES,\n",
    "  num_cpus_per_node=num_cpu_cores_per_worker,\n",
    "  num_gpus_per_node=num_gpu_per_worker,\n",
    "  collect_log_to_path=ray_log_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "151a6b68-a2e9-43aa-b652-06e040a7cf00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ray.init(address='auto', ignore_reinit_error=True)\n",
    "\n",
    "cluster_resources = ray.cluster_resources()\n",
    "print(cluster_resources)\n",
    "\n",
    "num_workers = int(cluster_resources[\"CPU\"] / num_cpu_cores_per_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df85edeb-f441-43b3-a8a0-f395f2766472",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###1.4. Loading the dataset\n",
    "You have to load [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset from Hugging Face Hub and split it into train and test datasets for fine-tuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9046cac5-593f-48ed-b4be-87290d35759a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\").train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=57\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24061640-230e-4668-aa0c-e93cfe3dc5f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###1.5. Creating the prompt template\n",
    "You have to create a prompt template function to generate prompts using the data features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbadebe-d838-4e6e-b430-d10d0c231401",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "\n",
    "PROMPT_NO_INPUT_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{response_key}\n",
    "{response}\n",
    "\n",
    "{end_key}\"\"\".format(\n",
    "  intro=INTRO_BLURB,\n",
    "  instruction_key=INSTRUCTION_KEY,\n",
    "  instruction=\"{instruction}\",\n",
    "  response_key=RESPONSE_KEY,\n",
    "  response=\"{response}\",\n",
    "  end_key=END_KEY\n",
    ")\n",
    "\n",
    "PROMPT_WITH_INPUT_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{input_key}\n",
    "{input}\n",
    "\n",
    "{response_key}\n",
    "{response}\n",
    "\n",
    "{end_key}\"\"\".format(\n",
    "  intro=INTRO_BLURB,\n",
    "  instruction_key=INSTRUCTION_KEY,\n",
    "  instruction=\"{instruction}\",\n",
    "  input_key=INPUT_KEY,\n",
    "  input=\"{input}\",\n",
    "  response_key=RESPONSE_KEY,\n",
    "  response=\"{response}\",\n",
    "  end_key=END_KEY\n",
    ")\n",
    "\n",
    "def apply_prompt_template(data):\n",
    "  instruction = data[\"instruction\"]\n",
    "  response = data[\"response\"]\n",
    "  context = data.get(\"context\")\n",
    "\n",
    "  if context:\n",
    "    full_prompt = PROMPT_WITH_INPUT_FORMAT.format(instruction=instruction, response=response, input=context)\n",
    "  else:\n",
    "    full_prompt = PROMPT_NO_INPUT_FORMAT.format(instruction=instruction, response=response)\n",
    "  \n",
    "  return { \"text\": full_prompt }\n",
    "\n",
    "def generate_prompt(prompt):\n",
    "    return apply_prompt_template(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e9f5ec0-e968-4d9f-9605-407578eb7183",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "You have to apply the prompt template function to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de76d3e-bf2b-43a9-9bbd-e648e84f7e6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hf_train_dataset = dataset[\"train\"].map(generate_prompt)\n",
    "hf_test_dataset = dataset[\"test\"].map(generate_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb961a34-74b8-44d1-b6df-a74650e21381",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Now, you have to convert the Hugging Face dataset into the Ray dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "038712a3-f6be-440e-9ff4-bb6603b834fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ray_train_dataset = ray.data.from_huggingface(hf_train_dataset)\n",
    "ray_test_dataset = ray.data.from_huggingface(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df7c0eac-b1c7-4222-b59d-890b7a40f4b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##2. Distributed Data Preprocessing\n",
    "\n",
    "###2.1. Implementing preprocessing function\n",
    "You need to define a preprocessing function to convert a batch of data to a format that the Llama 2 model can accept. Ray AIR `BatchMapper` will then map this function onto each incoming batch during the fine-tuning.\n",
    "\n",
    "The most important component is the tokenizer, which is a Hugging Face component associated with the Llama 2 model that turns natural language into formatted tokens with the right padding and truncation necessary for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ab0b882-1705-4dda-8b5f-7816c91d56df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "hf_token =  '<add your access token here>' # You need an access token\n",
    "max_length = 2048 # This is an appropriate max length according to the dataset\n",
    "batch_size = 4096\n",
    "\n",
    "login(token=hf_token)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_function(batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    result = tokenizer(\n",
    "        list(batch[\"text\"]),\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    " \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return dict(result)\n",
    "\n",
    "batch_preprocessor = BatchMapper(preprocess_function, batch_format=\"pandas\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "307bbcc2-7d4a-4005-9546-a7a308563425",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##3. Distributed Fine-tuning\n",
    "\n",
    "Each worker node contains a copy of the batch preprocessor to process partitioned batches of the Ray dataset, and individual model copies to train on these batches. PyTorch DDP synchronizes their weights, resulting in an integrated, fine-tuned model.\n",
    "\n",
    "###3.1. Creating an experiment in MLflow\n",
    "You have to first create an experiment in MLflow to register and save the fine-tuned model along with its artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85db5361-8c7f-41d2-bb02-c1dd1f02679e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment_name = \"exp-llama2-13b-hf\"\n",
    "experiment_location = f\"/Users/{username}/LLM/{experiment_name}\"\n",
    "artifact_location = f\"dbfs:/Users/{username}/{experiment_name}/artifacts\"\n",
    "\n",
    "tags = {\n",
    "    \"base_model_name\": model_name,\n",
    "    \"n_gpus\": str(num_workers),\n",
    "    \"num_cpu_cores_per_worker\": str(num_cpu_cores_per_worker),\n",
    "    \"num_gpu_per_worker\": str(num_gpu_per_worker),  \n",
    "    \"max_length\": str(max_length),\n",
    "}\n",
    "\n",
    "if not mlflow.get_experiment_by_name(experiment_location):\n",
    "    mlflow.create_experiment(name=experiment_location, artifact_location=artifact_location)\n",
    "\n",
    "mlflow.set_experiment(experiment_name=experiment_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d024f38-1f14-4ec9-bb2e-07eceee20954",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###3.2. Initializing trainer function for each worker\n",
    "The `trainer_init_per_worker` function creates a Hugging Face Transformers Trainer that gets distributed by Ray using Distributed Data Parallelism (using PyTorch Distributed Data Parallel). Each worker has its own copy of the model, however, it operates on a different batch of data. At the end of each step, all the workers sync gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6590b82-6b4d-4a6f-8dc0-a102191ee8fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def trainer_init_per_worker(\n",
    "    train_dataset: ray.data.Dataset,\n",
    "    eval_dataset: Optional[ray.data.Dataset] = None,\n",
    "    **config,\n",
    ") -> Trainer:\n",
    "    device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    login(token=config.get(\"hf_token\"))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=config.get(\"model_name\"), \n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"left\",\n",
    "        add_eos_token=True,\n",
    "        add_bos_token=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    max_memory = config.get(\"max_memory\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=config.get(\"model_name\"),\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        max_memory={i: max_memory for i in range(num_gpus)},\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=config.get(\"lora_alpha\", 16),\n",
    "        lora_dropout=config.get(\"lora_dropout\", 0.1),\n",
    "        r=config.get(\"lora_r\", 64),\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "                \"lm_head\",\n",
    "            ] # Choose all linear layers from the model\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=\"checkpoints\",\n",
    "        per_device_train_batch_size=config.get(\"per_device_train_batch_size\", 1),\n",
    "        gradient_accumulation_steps=config.get(\"gradient_accumulation_steps\", 1),\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_steps=config.get(\"save_steps\", 50),\n",
    "        logging_steps=config.get(\"logging_steps\", 50),\n",
    "        learning_rate=config.get(\"learning_rate\", 2e-4),\n",
    "        bf16=True,  # set True if you're using A10G or A100 GPU otherwise False\n",
    "        fp16=False, # set True if you're using V100 or T4 GPU otherwise False\n",
    "        max_grad_norm=0.3,\n",
    "        max_steps=config.get(\"max_steps\", 1000),\n",
    "        weight_decay=config.get(\"weight_decay\", 0.001),\n",
    "        logging_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        warmup_ratio=config.get(\"warmup_ratio\", 0.03),\n",
    "        group_by_length=False,\n",
    "        lr_scheduler_type=config.get(\"lr_scheduler_type\", \"constant\"),\n",
    "        ddp_find_unused_parameters=False,\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        args=training_arguments,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5522cc8b-32bc-4ce1-b6e1-cc68d1ec0b3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###3.3. Defining Transformers trainer\n",
    "Ray Transformers Trainer integrates with the Hugging Face Transformers library to scale training and fine-tuning across multiple workers, each with its own copy of the Hugging Face `transformers.Trainer` set up in the previous step.\n",
    "\n",
    "The following parameters need to be specified:\n",
    "- **`trainer_init_per_worker`** - Training function copied onto each worker.\n",
    "- **`trainer_init_config`** - Training configuration copied onto each worker.\n",
    "- **`scaling_config`** - Specify how to scale and the hardware to run on.\n",
    "- **`datasets`** - Specify which datasets to run training and evaluation on.\n",
    "- **`run_config`** - Specify checkpointing behaviour (how many times to save the model and how to compare between saved models).\n",
    "- **`preprocessor`** - The same Ray AIR preprocessor defined above is used to transform raw data into tokenized batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc8827c2-98bc-45e2-94cf-d7151519c734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.huggingface import TransformersTrainer\n",
    "\n",
    "max_memory = f'{23000}MB' # Memory of each GPU per worker\n",
    "logging_steps = 100\n",
    "save_steps = 100\n",
    "max_steps = 3000\n",
    "\n",
    "trainer = TransformersTrainer(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    trainer_init_config={\n",
    "        \"model_name\": model_name,\n",
    "        \"hf_token\": hf_token,\n",
    "        \"max_memory\": max_memory,\n",
    "        \"logging_steps\": logging_steps,\n",
    "        \"save_steps\" : save_steps,\n",
    "        \"max_steps\": max_steps\n",
    "    },\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=num_workers, \n",
    "        use_gpu=use_gpu,\n",
    "        resources_per_worker={\"GPU\": num_gpu_per_worker, \"CPU\": resource_per_worker_int}\n",
    "    ),\n",
    "    datasets={\n",
    "        \"train\": ray_train_dataset\n",
    "    },\n",
    "    run_config=RunConfig(\n",
    "        local_dir=f\"/local_disk0/user/{username}/hf\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "        callbacks=[\n",
    "            MLflowLoggerCallback(\n",
    "                experiment_name=experiment_location,\n",
    "                tags=tags,\n",
    "                save_artifact=True\n",
    "            )\n",
    "        ],\n",
    "        verbose=0\n",
    "    ),\n",
    "    preprocessor=batch_preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2ee2993-3c39-4183-b87f-27508164b44f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###3.4. Starting fine-tuning\n",
    "Finally, you have to call the `fit()` method to start fine-tuning with Ray AIR. It returns a result object which has metrics and checkpoint information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dcd668b-94d4-4cae-911e-c58cea56da40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "748c6b04-a145-4baf-af0b-33a381532372",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(result.checkpoint)\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43bb32e3-465e-4677-89b4-ae8b000a789f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###3.5. Merging weights and trying out the fine-tuned model\n",
    "Once you have the fine-tuned weights, just merge them with the pre-trained model. You have a memory-efficient fine-tuned model and tokenizer ready for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae449f5a-da63-43d3-9d47-e8adad4ff527",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "local_checkpoint = result.checkpoint\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    local_checkpoint.path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d6bb7ea-323b-4459-839b-4c1d082cfa27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
    "\n",
    "# Specify input\n",
    "text = \"### Instruction:\\nWhat are different ways people help each other?\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Get the generated text output\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"].to(device),\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_new_tokens=128, # Adjust max_new_tokens variable as you wish (maximum number of tokens the model can generate to answer the input)\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode output & print it\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b88c177-9a20-4bd3-b22c-de32e5737915",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###3.6. Start MLflow run to log model\n",
    "You have to log the fine-tuned model in MLflow using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baa2bcf6-21be-4945-9365-a05d94d02d7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"llama-2-13b-hf-finetuned\") as run:\n",
    "    pipe = transformers.pipeline(\n",
    "        task=\"text-generation\", \n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        framework=\"pt\",\n",
    "        torch_dtype=torch.bfloat16, \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    pip_reqs = mlflow.transformers.get_default_pip_requirements(model)\n",
    "    mlflow.transformers.log_model(\n",
    "        artifact_path=\"model\", \n",
    "        transformers_model=pipe, \n",
    "        pip_requirements=pip_reqs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb3b8194-6eca-4836-9aaa-7ab28d311824",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###3.7. Clearing CUDA memory in PyTorch.\n",
    "You can free up the GPU resources by running the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b955db2-1052-4766-ae24-9d5abfec5337",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "del model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acfd3231-42ca-4bdf-9801-796fd51f6682",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###3.8. Registering the model in MLflow\n",
    "You have to register and save the fine-tuned model in MLflow to load  it again later for batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf757c28-962e-42cd-8028-acb52b9d5fbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name_registry = \"llama-2-13b-hf-finetuned\"\n",
    "model_registered = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{run.info.run_id}/model\", \n",
    "    name=model_name_registry, \n",
    "    await_registration_for=600\n",
    ")\n",
    "\n",
    "print(f\"Model version {model_registered.version} has been registered.\")\n",
    "\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name_registry,\n",
    "    version=model_registered.version,\n",
    "    stage=\"Staging\",\n",
    "    archive_existing_versions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e143662-46dd-4f47-981b-6cdfef06ae78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##4. Distributed Hyperparameter Tuning (optional)\n",
    "You can perform hyperparameter tuning by running multiple trial experiments to find the better-performing fine-tuned model.Â \n",
    "\n",
    "###4.1. Defining Tuner\n",
    "You have to pass the previous TransformersTrainer object into Ray AIR  Tuner and configure the parameter search space and behavioural settings for scheduling, scaling, and checkpointing.\n",
    "\n",
    "There are four major components passed into the Tuner:\n",
    "- **`trainer`** - The TransformersTrainer with scaling, preprocessing, and fine-tuning logic from earlier.\n",
    "- **`param_space`** - The possibilities of hyperparameters to tune and search for any given trial.\n",
    "- **`tune_config`** - Specify how to compare different experiments, the number of trials, as well as any advanced search algorithms and schedulers like ASHA.\n",
    "- **`run_config`** - Used to specify checkpointing behaviour, custom callbacks, failure/retry configurations, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3d2af6b-1407-419c-95e0-39b4db8649f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune import Tuner\n",
    "from ray.tune.schedulers.async_hyperband import ASHAScheduler\n",
    "\n",
    "total_num_trials = 4\n",
    "max_tune_steps = 3000\n",
    "\n",
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space={\n",
    "        \"trainer_init_config\": {\n",
    "            \"learning_rate\": tune.choice([2e-5, 2e-4, 2e-3, 2e-2]),\n",
    "            \"max_steps\": tune.choice([1200, 1800, 2400, max_tune_steps]),\n",
    "            \"weight_decay\": tune.choice([0.01, 0.1, 1.0, 10.0]),\n",
    "        }\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=total_num_trials,\n",
    "        scheduler=ASHAScheduler(\n",
    "            max_t=max_tune_steps,\n",
    "        ),\n",
    "    ),\n",
    "    run_config=RunConfig(\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "        callbacks=[\n",
    "            MLflowLoggerCallback(\n",
    "                experiment_name=experiment_location,\n",
    "                tags=tags,\n",
    "                save_artifact=True\n",
    "            )\n",
    "        ],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdf6cc96-45d3-4823-af8b-5731576f5af2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###4.2. Starting hyperparameter tuning\n",
    "Finally, you have to call the `fit()` method to start hyperparameter tuning with Ray AIR. It returns a result object which has metrics and checkpoint information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb7e1eba-fe70-445d-8064-d98151838acc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tune_results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d665d025-ba89-4622-a48c-be3d7596cc94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tune_results_df = tune_results.get_dataframe().sort_values(\"loss\")\n",
    "tune_results_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2b35474-1efc-414b-a021-cd4e25128bab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_result = tune_results.get_best_result()\n",
    "\n",
    "print(best_result.checkpoint)\n",
    "print(best_result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ce87daa-d1f9-4894-999b-876cd680ad97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##5. Distributed Batch Inference\n",
    "You can perform distributed batch inference using Ray AIR `BatchPredictor` which applies batches of inputs to the fine-tuned model to generate predictions at scale.\n",
    "\n",
    "###5.1. Loading fine-tuned model\n",
    "You have to first load the fine-tuned model from the MLflow model repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ac2ae4b-323e-41e3-a4ba-818eca97a234",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n",
    "\n",
    "model_name_registry = \"llama-2-13b-hf-finetuned\"\n",
    "model_uri = f\"models:/{model_name_registry}/Staging\"\n",
    "\n",
    "local_artifact_path = ModelsArtifactRepository(model_uri).download_artifacts(artifact_path=\"pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2965525f-3219-49d3-ae62-ebceb3b116cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###5.2. Creating batch predictor\n",
    "You have to create a Ray AIR `BatchPredictor` from `TransformersCheckpoint` and specify the predictor class, task and some additional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25789291-7982-445b-8574-a50fdb2599f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ray.train.huggingface import TransformersCheckpoint, TransformersPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "\n",
    "local_checkpoint = TransformersCheckpoint.from_directory(local_artifact_path)\n",
    "\n",
    "predictor = BatchPredictor.from_checkpoint(\n",
    "    checkpoint=local_checkpoint, \n",
    "    predictor_cls=TransformersPredictor,\n",
    "    task=\"text-generation\",\n",
    "    use_gpu=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07e6aea6-f781-4390-a652-6d75d1d58abc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###5.3. Running batch inference\n",
    "You have to use the checkpoint to run batch prediction with `TransformersPredictor` which wraps around Hugging Face pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6650b9ad-fea7-469b-86c3-5b9ab2f6fbb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_response(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "    batch = batch[\"text\"].replace(to_replace='(### Response:|Input:)(\\s+)?(\\n.*)+', value='', regex=True)\n",
    "    return pd.DataFrame(batch, columns=[\"text\"])\n",
    "\n",
    "predictions = predictor.predict(\n",
    "    ray_test_dataset.map_batches(remove_response, batch_format=\"pandas\"),\n",
    "    batch_size=4096,\n",
    "    min_scoring_workers=1,\n",
    "    max_scoring_workers=3,\n",
    "    num_gpus_per_worker=1,\n",
    "    feature_columns=[\"text\"],\n",
    "    max_new_tokens=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9f680bb-0977-4658-9033-b55583a86826",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_df = predictions.to_pandas()\n",
    "predictions_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Llama-2-13b-hf_finetuning_with_ray_and_databricks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
